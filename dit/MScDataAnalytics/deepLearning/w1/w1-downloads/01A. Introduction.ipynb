{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Course - Introduction\n",
    "\n",
    "This is the introduction to the Deep Learning course. \n",
    "\n",
    "In this week's entry we will cover: \n",
    "\n",
    " * Course Scope \n",
    " * Prerequisites \n",
    " * Course Structure \n",
    " * Assignments and Assessment\n",
    " * Coding\n",
    " * Server Environment \n",
    "\n",
    "## Course Scope \n",
    "\n",
    "In this Deep Learning course we will trace through the most important concepts in Deep Learning such as the Backpropagation Algorithm, Convolutional Neural Networks, Recurrent Neural Networks and Restricted Boltzman Machines.\n",
    "\n",
    "The course will place emphasis on introducing concepts that are commonly used in practical Deep Learning research and application. An emphasis on running examples will be made. As such lecture notes will be provided in the style of Jupyter notebooks that a student can download, edit and run on their own machines. \n",
    "\n",
    "Some content commonly found in other courses will not be covered here. For example the following will not be addressed: \n",
    "\n",
    " * The history of neural networks \n",
    " * The biological inspiration of neural networks \n",
    "\n",
    "The interested student is directed to the many good external resources such as Geoff Hinton's online course for  detail on these topics. \n",
    "\n",
    "Speaking of videos, this course is not intended to be self-contained. While detailed notes with working examples will be given from week to week as Jupyter notebooks, links to external resources such as videos from Geoff Hinton and Andrew Ng will also be suggested for additional coverage or for a different perspective on a given question. \n",
    "\n",
    "## Prerequisites \n",
    "\n",
    "This course is intended as an Introduction to Deep Learning for students who have already completed undergraduate or Masters level modules on:\n",
    "\n",
    " * Artificial Intelligence\n",
    " * Machine Learning\n",
    " \n",
    "In the first few weeks the essential background topics of Linear and Logistic Regression will be re-introduced, but no other background Machine Learning concepts will be addressed such as distinctions between learning types; or benchmarking machine learning performance. Students who have no background in Machine learning should first take the Machine Learning module in the MSc programme. Students who have completed a Machine Learning module but feel rusty should consult with John Kelleher's textbook or Andrew Ng's free online Introduction to Machine Learning video series. \n",
    "\n",
    "The module is suitable for someone who has already taken an online module in Deep Learning or otherwise consulted books etc. While no new material will be covered the course will give you an opportunity to reflect on important topics with the support of fellow students. \n",
    "\n",
    "All assignments must be coded up using Python with related libraries. If you have not already coded in Python now is a good time to start. Introductory tutorials to Python and the use of the SKLearn (scikit) package should be consulted. In short make sure that you can load a data set, perform k-fold cross validation of an SVM based classifier, and then run and present metrics such as the F1 score. \n",
    "\n",
    "## Course Structure \n",
    "\n",
    "This is a 13 class programme with the following structure for the Spring 2018 semester. The course structure is subject to change and this content should be considered indicative. \n",
    "\n",
    "###### Class 1 - Introduction and Linear Regression\n",
    " * Course Scope \n",
    " * Prerequisites \n",
    " * Course Structure \n",
    " * Assignments and Assessment\n",
    " * Coding \n",
    " * Server Environment \n",
    " * Fitting functions to data\n",
    " * Cost Functions \n",
    " * Gradient Descent \n",
    " * Normalization of Data\n",
    " \n",
    "###### Class 2 - Logistic Regression\n",
    " * Non-linear Linear Regression\n",
    " * Regularization\n",
    " * The Logistic Function\n",
    " * The Cost Function for Logistic Units\n",
    " * Limits for Logistic Function\n",
    " * Higher Order Functions\n",
    " * Regularization for Logistic Regression\n",
    " \n",
    "######  Class 3 - Neural Network Representations and Forward Propogation\n",
    " * From Linear to Non-Linear Classifiers\n",
    " * Units\n",
    " * Layers\n",
    " * Bias Units\n",
    " * Building non-linear functions\n",
    " * The Feed Forward Algorithm\n",
    " \n",
    "###### Class 4 - Backpropagation\n",
    " * Overview of Backpropagation Methods\n",
    " * Deriving the Backpropagation Equations\n",
    " * The Backpropagation Algorithm\n",
    " * Visualizing Backpropogation in Tensorflow\n",
    " \n",
    "###### Class 5 - Refining Backpropagation\n",
    " * Cross Entropy Loss function \n",
    " * Hyperbolic Tangent Units\n",
    " * Rectified Linear Units\n",
    " * Softmax Layers \n",
    " \n",
    "###### Class 6 - Preventing Overfitting\n",
    " * Regularization in Neural Networks\n",
    " * Early Stopping\n",
    " * Dropout\n",
    " \n",
    "###### Class 7 - Convolutional Neural Networks\n",
    " * Test\n",
    " * Convolutions \n",
    " * Pooling Layers\n",
    " * Implementing a CNN\n",
    " * Scaling networks with a GPU\n",
    "\n",
    "###### Class 8 - Convolutional Neural Networks pt 2\n",
    " * Practical Applications\n",
    " * Assignment Review\n",
    " * Advanced CNN Applications\n",
    "\n",
    "###### Class 9 - Recurrent Neural Networks I\n",
    " * Basic topology\n",
    " * Motivating examples\n",
    " * Long Short Term Memory\n",
    " * Language Modelling Eample\n",
    " \n",
    "###### Class 10 - Recurrent Neural Networks II\n",
    " * Gated Recurrent Units\n",
    " * Dialogue Systems Example \n",
    " \n",
    "###### Class 11 - Unsupervised Training I\n",
    " * Motivation\n",
    " * Representations and Dimensionality Reduction\n",
    " * Autoencoders\n",
    " \n",
    "###### Class 12 - Unsupervised Training II\n",
    " * Restricted Boltzman Machines\n",
    " * RBM Training\n",
    " \n",
    "###### Week 13 - Class Test \n",
    " * Final Class Test \n",
    "\n",
    "Classes will be delivered approximately weekly on TUESDAY evening at 6pm. \n",
    "\n",
    "### Delivery Model\n",
    "\n",
    "The course is a 5 ECTS course at Level 9/10. There is two contact hours per week, but students are expected to engage in a significant amount of self-study each week. The contact hours will be used for group discussion, tests, reviewing assignments, and addressing any issues with respect to the delivery of the course. With the exception of Week 1, the hours are not used to deliver material. Instead students are expected to study the content for each module in advance of that class. The example for the class in Week 2, it is expected that students will have studied the course notes on Linear Regression, reviewed any relevant videos, and completed any relevant questions or assignments. Thus the class is delivered with a **flipped classroom** model. All content will be made available in the Deep Learning module on Webcourses. \n",
    "\n",
    "## Assignments and Assessment\n",
    "This is a 100\\% Continuous Assessment course. The assessment of the course will be broken down as follows:\n",
    "\n",
    " * 40\\% on in-class tests\n",
    " * 60\\% on Coding Challenge \n",
    "\n",
    "There will be two in-class tests to encourage students to continuously engage with the material and take on assignments. \n",
    "\n",
    "The Challenge / Project is a coding / modeling task which will allow the student to demonstrate a clear understanding of the concepts covered in the course. A dataset will be provided and students will be required to submit operational code and a short but detailed report on their model. Specific instructions on both the Challenge  will be provided. The Challenge will be due at the end of the semester. \n",
    "\n",
    "## Coding \n",
    "In this course we will use Python extensively for all examples and assignments. Rather than using vanilla Python we will where appropriate make use of Python packages that provide enhanced functionality for numerical computing and Deep Learning. Examples have been coded up using Python 3 rather than Python 2.7. it is possible to run these environments in Python 2.7 but in most cases imports from `__future__` will be necessary. \n",
    "\n",
    "The following packages are some of the most frequently used in this course:\n",
    " * **numpy**\n",
    " * **scipy** - often referred to as SciKit\n",
    " * **matplotlib**\n",
    "\n",
    "If you aren't familiar with either Python or these specific packages, now is the time to get familiar. \n",
    "\n",
    "There are many other interesting packages for numerical computing in Python. One interesting one is **Pandas**. This provides a frame based indexing mechanism similar to the one you may be used to in the R programming language. Pandas examples can in may cases be more intuitive but there is an overhead in computing based on Pandas frames. For this reason our examples will be based on numpy. \n",
    "\n",
    "Course notes and all examples will be coded up in the **Jupyter Notebook** environment. Notebooks will be made available on Webcourses for download. Students who do not already use Jupyter Notebook should install Jupyter Notebook. \n",
    "\n",
    "As indicated, the course will place emphasis on practical understanding - with attention placed on both the practical implementation and use of important model types. As such models will be explained with two types of examples: **The Hard Way** and **The Easy Way**. \n",
    "\n",
    "### The Hard Way\n",
    "\n",
    "In each week most material will explain how to implement key concepts from first principles. We refer to this way of doing things as **The Hard Way**. In these examples we will make use of the **numpy** library for performing numerical operations such as matrix multiplication or transposition. However we will in general be designing and coding important concepts such as neuron types, the backpropogation algorithm etc. with little reliance on well known libraries. The emphasis here will be on understanding how the algorithm works. In these cases many simplifying assumptions will often be made.\n",
    "\n",
    "### The Easy Way\n",
    "\n",
    "In each week we will also use examples to show how the newly introduced concept can be quickly implemented using well known 3rd party libraries such as **scipy** or **TensorFlow**, or **PyTorch**. The emphasis here will often be on more complex examples which demonstrate the true power or limitations of the models that we are investigating.\n",
    "\n",
    "## Server Environment \n",
    "\n",
    "Students are expected to have running installations of scipy and TensorFlow running on their own machines for testing. Where possible you should use a machine with a GPU and test out your code on both CPU and GPU architectures. \n",
    "\n",
    "However for your Assignment 1 and Assignment 2 task each student will be given some time on one of our GPU nodes to run tests on their code. Running code will be deployed using **Slurm** to avoid conflicts. Keep in mind that only one node will be made available between all students. Therefore it will be necessary for you to test your code completely in advance and be sure to provide yourself with sufficient time for testing given that there may be a significant draw on the machines by other students. \n",
    "\n",
    "Details on the server environment including the IP address of your machine, your username and password, and details on how to deploy a process with **Slurm** will be detailed when the first assignment is given out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendices \n",
    "\n",
    "Below you will find a couple of useful reference posts for issues such as notation and linear algebra in python. \n",
    "\n",
    "## Appendix A - Notation\n",
    "\n",
    "Given a training set we talk about:\n",
    " * $m$ = number of training examples\n",
    " * $x$'s = input variables or features\n",
    " * $y$'s = output variable or target\n",
    " * $(x,y)$ = one training example\n",
    " * $(x^{i},y^{i})$ = refers specifically to the ith training case\n",
    "\n",
    "## Appendix B - Linear Algebra in Python\n",
    "\n",
    "In python we can use the `numpy` library to easily define and perform operations on vectors and matrices. \n",
    "\n",
    "### Matrices\n",
    "The term **matrix** refers to a 2D rectangular array of numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We can define a matrix directly from a string of numbers where rows are delimited by semi-colons\n",
    "A = np.matrix('1 2 3; 4 5 6')\n",
    "print(A)\n",
    "\n",
    "# Alternatively we can define the same matrix from a series of vectors where each vector defines a row of the matrix\n",
    "B = np.matrix([[1, 2, 3], [4, 5, 6]])\n",
    "print(B)\n",
    "\n",
    "# Alternatively we can use numpy's array constructor to creaea a 2D array, i.e., our matrix\n",
    "B = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix will have a certain *dimensionality* defined in terms of the number of rows and the number of columns. We can use standard number theory notation to define the dimensionality of the matrix. For example we can define the set of matrices of real numbers with 3 columns and 2 rows as $R^{3x2}$. \n",
    "\n",
    "The **shape** function in numpy will return the number of rows and columns for a given matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We refer to individual objects within the matrix as **elements**. We use subscript notation on the matrix name in order to refer to individual objects. In general $M_{i,j}$ will refer to the element found on the $i^{th}$ row and $j^{th}$ column of M. \n",
    "\n",
    "Numpy supports a wide range of methods for indexing and slicing arrays which we will not cover here. In the simple case we can however use indices to operate directly on the matrix as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(A[1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that indexing on the rows and columns of numpy matrices begins at 0. \n",
    "\n",
    "### Vectors\n",
    "A vector is a 1D array and as such can be thought of as a special case of a matrix with only one column. \n",
    "\n",
    "While the vector is in general a special case of a matrix, we typically use different operators to create and work with vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 1 0]\n",
      "[[2 3 1 0]]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# create a numpy vector by way of a stadard python list fed into the array constructor\n",
    "v1 = np.array([2,3,1,0])\n",
    "print(v1)\n",
    "\n",
    "# note that this is not equivilent to attempting to create the array as a single row of a matrix\n",
    "v2 = np.matrix([[2,3,1,0]])\n",
    "print(v2)\n",
    "\n",
    "# index an element in the vector\n",
    "print(v1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix and Vector Basic Operations\n",
    "\n",
    "We can add and subtract matrices which are of the same dimensionality to result in a new matrix which is of that same dimensionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  4  6]\n",
      " [ 8 10 12]]\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "C = A + B\n",
    "print(C)\n",
    "D = C - A\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12 23 31 40]\n",
      "[ 8 17 29 40]\n"
     ]
    }
   ],
   "source": [
    "v2 = np.array([10,20,30,40])\n",
    "\n",
    "v3 = v1 + v2\n",
    "print(v3)\n",
    "v3 = v2 - v1\n",
    "print(v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also directly apply scalar multiplication and division operations to matrices and vectors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  4  6]\n",
      " [ 8 10 12]]\n",
      "[[0.5 1.  1.5]\n",
      " [2.  2.5 3. ]]\n",
      "[20 40 60 80]\n",
      "[ 5. 10. 15. 20.]\n"
     ]
    }
   ],
   "source": [
    "E = A * 2\n",
    "print(E)\n",
    "F = A / 2\n",
    "print(F)\n",
    "v4 = v2 * 2\n",
    "print(v4)\n",
    "v5 = v2 / 2\n",
    "print(v5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner / Scalar Product\n",
    "The Inner Product or Scalar Product of two vectors is the scalar result of summing the pairwise products of elements in two vectors of equal length. In geometric space the Scalar Product is often interpreted as a distance metric between two points in that space. This is often used for example to calculate document similarities. \n",
    "\n",
    "In python we can calculate the scalar product of two vectors using numpy's inner function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    }
   ],
   "source": [
    "v3 = np.dot(v1,v2)\n",
    "print(v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the dot function will also product this result when applied to two vectors. However the dot function can also be applied to matrices and higher-order arrays. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix-Matrix Multiplication\n",
    "Given two matrices we can calculate the cross product of these matrices so long as the number of rows in the first matrix equals the number of columns in the second. \n",
    "\n",
    "For two matrices A and B, the dimensionality of the resultant matrix product is given by: \n",
    "\n",
    "\\begin{equation}\n",
    "R_{A}C_{A} \\times R_{B}C_{B} = R_{A}C_{B}\n",
    "\\end{equation}\n",
    "\n",
    "The operation for calculating the matrix product is straightforward: the entry for row i column j in the resultant matrix C is the dot product of the i$^{th}$ row of A and the j$^{th}$ column of B. \n",
    "\n",
    "![matrix matrix multiplication](figures/img792.gif)\n",
    "\n",
    "In python we can use the numpy function **matmul** to perform matrix-matrix multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[[22 28]\n",
      " [49 64]]\n"
     ]
    }
   ],
   "source": [
    "G = np.matrix('1 2; 3, 4; 5, 6')\n",
    "print(G)\n",
    "H = np.matmul(A,G)\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that matrix matrix multiplication is not commutative. \n",
    "\\begin{equation}\n",
    "  A \\times B \\neq B \\times A\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "[[1 1 1]\n",
      " [2 2 2]\n",
      " [3 3 3]]\n",
      "[[14 14 14]\n",
      " [32 32 32]\n",
      " [50 50 50]]\n",
      "[[12 15 18]\n",
      " [24 30 36]\n",
      " [36 45 54]]\n"
     ]
    }
   ],
   "source": [
    "A = np.matrix('1, 2, 3; 4, 5, 6; 7, 8, 9')\n",
    "B = np.matrix('1, 1, 1; 2, 2, 2; 3, 3, 3')\n",
    "print(A)\n",
    "print(B)\n",
    "print(np.matmul(A,B))\n",
    "print(np.matmul(B,A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Identity and Inverse \n",
    "\n",
    "We can define the identity $I$ of a matrix which in general allows the commutation property to hold. \n",
    "\n",
    "\\begin{equation}\n",
    " A \\times I = I \\times A = A\n",
    "\\end{equation}\n",
    "\n",
    "Here $I$ is the Identity Matrix which is a square matrix where all diagonal elements are = 1 and all non-diagonal elements are = 0. \n",
    "\n",
    "Numpy allows us to easily define an identity matrix with a specified number of rows and columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n"
     ]
    }
   ],
   "source": [
    "I = np.identity(3)\n",
    "print(I)\n",
    "print(A)\n",
    "print(np.matmul(A,I))\n",
    "print(np.matmul(I,A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we can define the inverse for a real number $X \\in R$ as $\\frac{1}{X}$, we can also define the inverse for a matrix. \n",
    "\n",
    "Beginning first with the case of real numbers, we note that: \n",
    "\n",
    "\\begin{equation}\n",
    " X \\times INV(X) = I\n",
    "\\end{equation}\n",
    "\n",
    "where 1 is the identity for real numbers - which is 1. \n",
    "\n",
    "This gives us an intuition of how the inverse is defined for matrices:  \n",
    "\n",
    "\\begin{equation}\n",
    " A \\times INV(A) = I \n",
    "\\end{equation}\n",
    "\n",
    "i.e., the inverse of a matrix A should be defined such that the matrix product of A by it produces an identity matrix. \n",
    "\n",
    "While we straightforwardly calculate the inverse of a real number X as $\\frac{1}{X}$, the calculation of the inverse of a matrix is more complicated and involves calculations of Determinants and Cofactors of matrices which we will not consider here. Fortunately we can of course calculate the inverse directly in numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00746988 -0.00349497  0.01506633]\n",
      " [-0.0023959   0.00211775  0.00542254]\n",
      " [-0.00056121  0.00191823 -0.00104746]]\n",
      "[[ 1.00000000e+00  4.31512465e-17  2.29850861e-17]\n",
      " [ 4.47775497e-17  1.00000000e+00 -5.16080234e-17]\n",
      " [ 4.32596667e-17 -3.10081821e-17  1.00000000e+00]]\n",
      "Uncomment lines in the block to run this - but expect an error!\n"
     ]
    }
   ],
   "source": [
    "import numpy.linalg as la\n",
    "X = np.matrix('100, -200, 403; 44, -5, 607; 27, 98, -59')\n",
    "B = la.inv(X)\n",
    "print(B)\n",
    "print(np.matmul(X,B))\n",
    "\n",
    "# note that the process doesn't work very well when the candidate matrix is close to 0\n",
    "print(\"Uncomment lines in the block to run this - but expect an error!\") \n",
    "# B = la.inv(A)\n",
    "# print(np.matmul(A,B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Transpose\n",
    "We can also define the transpose of a matrix $A^{T}$ as a matrix such that rows and columns of $A$ are reversed. \n",
    "\n",
    "\\begin{equation}\n",
    "   A_{ij} = A_{ji}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "[[1 4 7]\n",
      " [2 5 8]\n",
      " [3 6 9]]\n"
     ]
    }
   ],
   "source": [
    "C = A.T\n",
    "print(A)\n",
    "print(C)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
